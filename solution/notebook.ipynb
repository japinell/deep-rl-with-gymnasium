{"cells":[{"source":"# Deep Reinforcement Learning with Gymnasium\n\nReinforcement Learning (RL) is one of the three main machine learning paradigms, alongside supervised and unsupervised learning. Unlike the other two, RL focuses on training an agent to interact with its environment by making decisions that maximize cumulative rewards. Through trial and error, the agent learns the optimal actions to take in different situations.\n\nA powerful extension of this approach is Reinforcement Learning with Human Feedback (RLHF), where human input helps refine the agent’s behavior at each step, leading to more aligned and effective decision-making.\n\nRL has a wide range of applications, from self-driving cars and automated trading to game-playing AI and robotic control. When combined with deep neural networks, it becomes Deep Reinforcement Learning, enabling breakthroughs in complex problem-solving.\n\nIn this code-along, we’ll dive into Gymnasium, an open-source Python library for developing and benchmarking RL algorithms. I’ll guide you through setting it up, exploring different RL environments, and implementing a simple agent to apply an RL algorithm in Python.\n\nLet’s get started! 🚀","metadata":{},"id":"53200e55-70d1-4325-813a-e94a0d9dc6dd","cell_type":"markdown"},{"source":"## What is Gymnasium?\n\n[Gymnasium](https://gymnasium.farama.org/) is an open-source Python library designed to support the development and evaluation of reinforcement learning (RL) algorithms. It provides a robust framework that simplifies RL research and experimentation by offering:\n\n- A diverse range of environments, from simple games to complex real-world simulations.\n- Intuitive APIs and wrappers for seamless interaction with environments.\n- Flexibility to create custom environments while leveraging the standardized API framework.\n\nWith Gymnasium, developers can easily build and test RL algorithms using API calls to:\n\n- Send the agent’s chosen actions to the environment.\n- Retrieve the environment’s state and reward after each action.\n- Train the RL model efficiently.\n- Evaluate the model’s performance in different scenarios.\n\nThis structured approach makes Gymnasium a powerful tool for both beginners and experienced researchers in RL.","metadata":{},"id":"afe48932-1ccb-4ec3-bac0-d5f4aedc9457","cell_type":"markdown"},{"source":"Since this code-along is recorded at a certain point in time, we'll install specific versions of the required dependencies.","metadata":{},"id":"1ff003b9-3502-4ead-83dc-389bfb244de1","cell_type":"markdown"},{"source":"!pip install torch==2.3.1 gymnasium==1.1.1","metadata":{"executionCancelledAt":null,"executionTime":3426,"lastExecutedAt":1742048559934,"lastExecutedByKernel":"994b845e-925f-415b-91a9-641a980bbd40","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torch==2.3.1 gymnasium==1.1.1","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"7d5a3839-01de-40f8-bd11-334d61d0a3d5","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\nCollecting gymnasium==1.1.1\n  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.1.1) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.1.1) (3.1.1)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.1.1) (0.0.4)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.8.61)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\nDownloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: gymnasium\nSuccessfully installed gymnasium-1.1.1\n"}]},{"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributions as distributions\nimport numpy as np\nimport gymnasium as gym  ","metadata":{"executionCancelledAt":null,"executionTime":3047,"lastExecutedAt":1742048572250,"lastExecutedByKernel":"994b845e-925f-415b-91a9-641a980bbd40","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributions as distributions\nimport numpy as np\nimport gymnasium as gym  "},"id":"3c158a68-ce89-4e36-87b1-b90e78c96978","cell_type":"code","execution_count":3,"outputs":[]},{"source":"## Task 1: Setting up a Gymnasium Environment\n\nA [Gymnasium Environment](https://gymnasium.farama.org/api/env/) is a controlled setting where an RL agent interacts, learns, and makes decisions to achieve a goal. Environments provide a structured way to model various real-world and simulated scenarios, making them essential for developing and testing reinforcement learning (RL) algorithms.","metadata":{},"id":"067703c1-e6d8-4a85-80b3-71c3f0a456ef","cell_type":"markdown"},{"source":"For this code-along we'll use the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment. Our goal is to develop a simple neural network that is able keep the inverted pendulumn upright by the control the left-to-right motion of the cart on which it stands.\n\nAn episode ends if one of the following conditions occur:\n\n1. Termination: Pole Angle is greater than ±12°\n2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n3. Truncation: Episode length is greater than 500 (200 for v0)\n\nWe'll specify `render_mode=\"rgb_array\"` to be able to visualize the state using matplotlib later on. \n\n![Cartpole](cartpole.png)\n","metadata":{},"id":"b966e177-dd69-4d22-a56f-4c93b0c9548c","cell_type":"markdown"},{"source":"","metadata":{},"cell_type":"code","id":"6b0077ba-5416-42da-8af1-87753a99e24f","outputs":[],"execution_count":null},{"source":"## Task 2: Create a Neural Network","metadata":{},"id":"41f9cb70-b976-4759-a068-d40e8008c6e4","cell_type":"markdown"},{"source":"","metadata":{},"cell_type":"code","id":"8607964e-5067-40db-94b3-8a86e4effe29","outputs":[],"execution_count":null},{"source":"## Task 3: Train and validate policy","metadata":{},"id":"5d3ff8e5-266c-4942-b3d6-459656cb2e9a","cell_type":"markdown"},{"source":"","metadata":{},"cell_type":"code","id":"45d0907d-779a-41ab-a82d-417f182935fd","outputs":[],"execution_count":null},{"source":"## Additional tasks:\n\n- Tweak the discount factor and evaluate the effects on training\n- Create an agent for other Gymnasium Environments\n- Try out different loss functions like the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)","metadata":{},"cell_type":"markdown","id":"80f92aad-9cab-42e2-b98f-a2612b01c8e7"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}